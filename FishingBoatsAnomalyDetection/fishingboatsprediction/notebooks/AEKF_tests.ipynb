{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e0e9f3",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81379a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from scipy.linalg import inv, cholesky\n",
    "from scipy import stats\n",
    "from scipy.stats import vonmises, wrapcauchy, norm, cauchy, uniform\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from fiona.crs import from_epsg\n",
    "\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "CRS_METRIC = from_epsg(4326)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e02118",
   "metadata": {},
   "source": [
    "## Plotting AIS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "cab485e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AISColumnNames:\n",
    "    Date: str = \"timestamp\"\n",
    "    Sampled_Date: str = \"sampled_timestamp\"\n",
    "    Latitude: str = \"latitude\"\n",
    "    Longitude: str = \"longitude\"\n",
    "    Pseudo_Longitude: str = \"pseudo_longitude\"\n",
    "    SOG: str = \"sog\"\n",
    "    COG: str = \"cog\"\n",
    "    Heading: str = \"heading\"\n",
    "\n",
    "    n_Latitude: str = \"norm_latitude\"\n",
    "    n_Longitude: str = \"norm_longitude\"\n",
    "    n_SOG: str = \"norm_sog\"\n",
    "    n_COG: str = \"norm_cog\"\n",
    "    n_Heading: str = \"norm_heading\"\n",
    "\n",
    "    is_synthetic: str = \"is_synthetic\"\n",
    "    to_predict: str = \"to_predict\"\n",
    "\n",
    "cols: AISColumnNames = AISColumnNames()\n",
    "target_freq_in_minutes = 10\n",
    "target_freq: str = f\"{target_freq_in_minutes}min\"\n",
    "sample_T: pd.Timedelta = pd.Timedelta(minutes=target_freq_in_minutes)\n",
    "\n",
    "\n",
    "def get_trajectory_sequences(trajectory_sampled: pd.DataFrame, time_column_name=None\n",
    "    ) -> List[pd.DataFrame]:\n",
    "        trajectory_sequences: List[pd.DataFrame] = []  # To store the sequences\n",
    "        current_sequence = pd.DataFrame(\n",
    "            columns=trajectory_sampled.columns\n",
    "        )  # DF To track the current sequence\n",
    "\n",
    "        # Iterate through the timestamps\n",
    "        for i in range(len(trajectory_sampled) - 1):\n",
    "            if (\n",
    "                trajectory_sampled.index[i + 1]\n",
    "                - trajectory_sampled.index[i]\n",
    "                == sample_T\n",
    "            ):\n",
    "                # If the difference is 10 minutes, add the current timestamp to the sequence\n",
    "                if len(current_sequence) == 0:\n",
    "                    current_sequence = trajectory_sampled.iloc[\n",
    "                        [i]\n",
    "                    ]  # Add the first timestamp of the sequence\n",
    "                current_sequence = pd.concat(\n",
    "                    [current_sequence, trajectory_sampled.iloc[[i + 1]]],\n",
    "                    # ignore_index=True,\n",
    "                )  # Add the next timestamp\n",
    "            else:\n",
    "                # If the difference is not 10 minutes, end the current sequence\n",
    "                if len(current_sequence) != 0:\n",
    "                    trajectory_sequences.append(\n",
    "                        current_sequence\n",
    "                    )  # Store the completed sequence\n",
    "                    current_sequence = pd.DataFrame(\n",
    "                        columns=trajectory_sampled.columns\n",
    "                    )  # Reset the current sequence\n",
    "\n",
    "        # Handle the last sequence if it ends at the last timestamp\n",
    "        if len(current_sequence) != 0:\n",
    "            trajectory_sequences.append(current_sequence)\n",
    "            \n",
    "        # handle one last point\n",
    "        if trajectory_sequences[-1].index[-1] != trajectory_sampled.index[-1]:\n",
    "            trajectory_sequences.append(trajectory_sampled.iloc[[-1]])\n",
    "            \n",
    "        # handle one first point\n",
    "        if trajectory_sequences[0].index[0] != trajectory_sampled.index[0]:\n",
    "            trajectory_sequences.append(trajectory_sampled.iloc[[0]])\n",
    "\n",
    "        return trajectory_sequences\n",
    "    \n",
    "def plot_plotly_trajectory_groups(df_groups: List[List[pd.DataFrame]],\n",
    "                         group_names, \n",
    "                         color_sequence=None,\n",
    "                         line_width=2,\n",
    "                         marker_size=4):\n",
    "    if not df_groups:\n",
    "        raise ValueError(\"Empty list of DataFrame groups provided\")\n",
    "    \n",
    "    if color_sequence is None:\n",
    "        color_sequence = px.colors.qualitative.Plotly\n",
    "    \n",
    "    # Create empty figure with proper mapbox setup\n",
    "    fig = px.scatter_mapbox(lat=[None], lon=[None]).update_layout(\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        mapbox_zoom=8,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    for group_id, df_group in enumerate(df_groups):\n",
    "        group_color = color_sequence[group_id % len(color_sequence)]\n",
    "        \n",
    "        for segment_id, df in enumerate(df_group):\n",
    "            if len(df) == 0:\n",
    "                continue  # Skip empty dataframes\n",
    "                \n",
    "            customdata = pd.concat([\n",
    "                pd.Series(df.index, name=cols.Sampled_Date, index=df.index),\n",
    "                df[cols.SOG],\n",
    "                df[cols.COG],\n",
    "                \n",
    "            ], axis=1)\n",
    "                        \n",
    "            # Add line trace for this segment\n",
    "            fig.add_trace(\n",
    "                px.line_mapbox(\n",
    "                    df,\n",
    "                    lat=cols.Latitude,\n",
    "                    lon=cols.Longitude,\n",
    "                    color_discrete_sequence=[group_color]\n",
    "                ).data[0].update(\n",
    "                    mode=\"lines+markers\",\n",
    "                    line=dict(width=line_width),\n",
    "                    marker=dict(size=marker_size),\n",
    "                    name=f\"{group_names[group_id]}\",\n",
    "                    showlegend=(segment_id == 0),  # Only show legend for first segment\n",
    "                    legendgroup=f\"{group_names[group_id]}\",\n",
    "                    hoverinfo=\"text\",\n",
    "                    customdata=customdata,\n",
    "                    hovertemplate=(\n",
    "                        \"Latitude: %{lat}<br>\"\n",
    "                        \"Longitude: %{lon}<br>\"\n",
    "                        \"Date: %{customdata[0]}<br>\"\n",
    "                        \"SOG: %{customdata[1]}<br>\"\n",
    "                        \"COG: %{customdata[2]}<br>\"\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "        showlegend=True,\n",
    "        legend_title_text=\"Trajectory Groups\",\n",
    "        title=\"Vessel Trajectory\"\n",
    "    )\n",
    "    \n",
    "    # Auto-zoom to the data\n",
    "    if len(df_groups) > 0 and len(df_groups[0]) > 0:\n",
    "        first_df = df_groups[0][0]\n",
    "        fig.update_mapboxes(\n",
    "            center=dict(\n",
    "                lat=first_df[cols.Latitude].mean(),\n",
    "                lon=first_df[cols.Longitude].mean()\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09bce7",
   "metadata": {},
   "source": [
    "## Old AEKF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f535346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssa(eps):\n",
    "    \"\"\"\n",
    "    Normalize latitude and longitude residuals to standard range.\n",
    "    \"\"\"\n",
    "    eps[0] = (eps[0] + np.pi/2) % np.pi - np.pi/2\n",
    "    eps[1] = (eps[1] + np.pi) % (2 * np.pi) - np.pi\n",
    "    return eps\n",
    "\n",
    "\n",
    "def AEKF_traj(traj: mpd.Trajectory, alpha: float = 0.2, mmsi_colName=\"mmsi\"):\n",
    "    \"\"\"\n",
    "    Apply Adaptive Extended Kalman Filter to correct maritime vessel trajectory.\n",
    "    \"\"\"\n",
    "    df = traj.df.copy()\n",
    "    mmsi = df[mmsi_colName].unique()[0]\n",
    "    \n",
    "    # WGS-84 ellipsoid constants\n",
    "    a = 6378137\n",
    "    f = 1 / 298.257223563\n",
    "    e = np.sqrt(2 * f - f**2)\n",
    "\n",
    "    alpha_1, alpha_2 = 0.01, 0.01\n",
    "\n",
    "    # Initial lat/lon in radians\n",
    "    lat_rad = np.deg2rad(df[cols.Latitude].values)\n",
    "    lon_rad = np.deg2rad(df[cols.Longitude].values)\n",
    "\n",
    "    # Covariances\n",
    "    Q11, Q22 = 1e8, 1e2\n",
    "    R11 = R22 = 36\n",
    "    Qd_base = np.diag([Q11, Q22])\n",
    "    Rd_base = np.diag([R11, R22])\n",
    "\n",
    "    P_prd = np.eye(5)\n",
    "    Cd = np.array([[1, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 0, 0]])\n",
    "\n",
    "    coord_ls, sog, cog = [], [], []\n",
    "    x_hat = np.array([lat_rad[0], lon_rad[0], 0, 0, 0], dtype=float)\n",
    "\n",
    "    dt_sec = np.diff(df.index.view(np.int64) // 1_000_000_000)\n",
    "    dt_min = dt_sec.min()\n",
    "    dts = np.insert(dt_sec, 0, 0)\n",
    "    \n",
    "    try:\n",
    "        for i in range(len(df)):\n",
    "            h = dts[i]\n",
    "            y = np.array([lat_rad[i], lon_rad[i]])\n",
    "\n",
    "            # Adaptive noise adjustment\n",
    "            if alpha and h > 0:\n",
    "                Qd = alpha * (h / dt_min) * Qd_base + (1 - alpha) * Qd_base\n",
    "                Rd = alpha * (dt_min / h) * Rd_base + (1 - alpha) * Rd_base\n",
    "            else:\n",
    "                Qd, Rd = Qd_base, Rd_base\n",
    "\n",
    "            Ed = h * np.array([[0, 0],\n",
    "                               [0, 0],\n",
    "                               [1, 0],\n",
    "                               [0, 0],\n",
    "                               [0, 1]])\n",
    "\n",
    "            Rn = a / np.sqrt(1 - e**2 * np.sin(x_hat[0])**2)\n",
    "            Rm = Rn * ((1 - e**2) / (1 - e**2 * np.sin(x_hat[0])**2))\n",
    "\n",
    "            f = np.array([\n",
    "                (1 / Rm) * x_hat[2] * np.cos(x_hat[3]) * h,\n",
    "                (1 / (Rn * np.cos(x_hat[0]))) * x_hat[2] * np.sin(x_hat[3]) * h,\n",
    "                -alpha_1 * x_hat[2],\n",
    "                x_hat[4] * h,\n",
    "                -alpha_2 * x_hat[4]\n",
    "            ])\n",
    "\n",
    "            A21 = (x_hat[2] * np.sin(x_hat[3]) * np.tan(x_hat[0])) / (Rn * np.cos(x_hat[0]))\n",
    "            Ad = np.eye(5) + h * np.array([\n",
    "                [0, 0, (1 / Rm) * np.cos(x_hat[3]), -(1 / Rm) * x_hat[2] * np.sin(x_hat[3]), 0],\n",
    "                [A21, 0, (1 / (Rn * np.cos(x_hat[0]))) * np.sin(x_hat[3]), (1 / (Rn * np.cos(x_hat[0]))) * x_hat[2] * np.cos(x_hat[3]), 0],\n",
    "                [0, 0, -alpha_1, 0, 0],\n",
    "                [0, 0, 0, 0, 1],\n",
    "                [0, 0, 0, 0, -alpha_2]\n",
    "            ])\n",
    "\n",
    "            # print(f)\n",
    "            x_prd = x_hat + f\n",
    "            P_hat = Ad @ P_prd @ Ad.T + Ed @ Qd @ Ed.T\n",
    "\n",
    "            d = ssa(y - Cd @ x_prd)\n",
    "            S = Cd @ P_hat @ Cd.T + Rd\n",
    "            K = P_hat @ Cd.T @ np.linalg.inv(S)\n",
    "\n",
    "            x_hat = x_prd + K @ d\n",
    "            x_hat[:2] = ssa(x_hat[:2])\n",
    "            IKC = np.eye(5) - K @ Cd\n",
    "            P_prd = IKC @ P_hat @ IKC.T + K @ Rd @ K.T\n",
    "\n",
    "            # Save result\n",
    "            coord_ls.append(np.rad2deg(x_hat[:2]))\n",
    "            sog.append(x_hat[2] / (0.51444444 * (h if h > 0 else 1)))  # m/s to knots\n",
    "            cog.append(np.rad2deg(x_hat[3]) % 360)\n",
    "\n",
    "        lat_deg, lon_deg = zip(*coord_ls)\n",
    "        df_corr = pd.DataFrame({cols.Latitude: lat_deg, cols.Longitude: lon_deg, cols.SOG: sog, cols.COG: cog}, index=df.index)\n",
    "        df_geo = gpd.GeoDataFrame(df_corr, geometry=gpd.points_from_xy(df_corr[cols.Longitude], df_corr[cols.Latitude]), crs=CRS_METRIC)\n",
    "        return mpd.Trajectory(df_geo, traj_id=mmsi)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in AEKF_traj: {e}\")\n",
    "        return False\n",
    "\n",
    "def RMSE_error(collection, new_collections_aekf, mask_traj):\n",
    "    \"\"\"\n",
    "    Calculate RMSE between true and filtered trajectories.\n",
    "    \"\"\"\n",
    "    RMSE_aekf = []\n",
    "    collection = [collection[i] for i, keep in enumerate(mask_traj) if keep]\n",
    "\n",
    "    for true_traj, est_traj in tqdm(zip(collection, new_collections_aekf), desc=\"RMSE\"):\n",
    "        true_coord = np.deg2rad(true_traj.df[[cols.Latitude, cols.Longitude]].values)\n",
    "        pred_coord = np.deg2rad(est_traj.df[[cols.Latitude, cols.Longitude]].values)\n",
    "        dist_matrix = haversine_distances(true_coord, pred_coord) * 6371  # km\n",
    "        diag_dist = np.diag(dist_matrix)\n",
    "        rmse = np.sqrt((diag_dist**2).mean())\n",
    "        RMSE_aekf.append(rmse)\n",
    "\n",
    "    return RMSE_aekf\n",
    "\n",
    "\n",
    "def AEKF_traj_advanced(traj: pd.DataFrame, alpha: float = 0.2, mmsi_colName=\"mmsi\", max_to_reconstruct=10):\n",
    "    \"\"\"\n",
    "    Apply Adaptive Extended Kalman Filter to correct maritime vessel trajectory.\n",
    "    \"\"\"\n",
    "    df = traj.copy()\n",
    "    mmsi = df[mmsi_colName].unique()[0]\n",
    "    \n",
    "    # WGS-84 ellipsoid constants\n",
    "    a = 6378137\n",
    "    f = 1 / 298.257223563\n",
    "    e = np.sqrt(2 * f - f**2)\n",
    "\n",
    "    alpha_1, alpha_2 = 0.01, 0.01\n",
    "\n",
    "    # Initial lat/lon in radians\n",
    "    lat_rad = np.deg2rad(df[cols.Latitude].values)\n",
    "    lon_rad = np.deg2rad(df[cols.Longitude].values)\n",
    "\n",
    "    # Covariances\n",
    "    Q11, Q22 = 1e8, 1e2\n",
    "    R11 = R22 = 36\n",
    "    Qd_base = np.diag([Q11, Q22])\n",
    "    Rd_base = np.diag([R11, R22])\n",
    "    delta_q = Qd_base\n",
    "    delta_r = Rd_base\n",
    "\n",
    "    P_prd = np.eye(5)\n",
    "    Cd = np.array([[1, 0, 0, 0, 0],\n",
    "                   [0, 1, 0, 0, 0]])\n",
    "\n",
    "    coord_ls, sog, cog, timestamps = [], [], [], []\n",
    "    x_hat = np.array([lat_rad[0], lon_rad[0], 0, 0, 0], dtype=float)\n",
    "\n",
    "    dt_sec = np.diff(df.index.view(np.int64) // 1_000_000_000)\n",
    "    dt_min = dt_sec.min()\n",
    "    dts = np.insert(dt_sec, 0, 0)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        for i in range(len(df)):\n",
    "            h = dts[i]\n",
    "            y = np.array([lat_rad[i], lon_rad[i]])\n",
    "            \n",
    "            n_steps = max(1, int(h / dt_min))\n",
    "            \n",
    "            if n_steps > max_to_reconstruct:\n",
    "                Qd, Rd = Qd_base, Rd_base\n",
    "                x_hat = np.array([lat_rad[i], lon_rad[i], 0, 0, 0], dtype=float)\n",
    "                coord_ls.append(np.rad2deg(x_hat[:2]))\n",
    "                sog.append(df[cols.SOG].iloc[i])  # m/s to knots\n",
    "                cog.append(df[cols.COG].iloc[i])\n",
    "                if len(timestamps) == 0:\n",
    "                    timestamps.append(df.index[0])\n",
    "                else:\n",
    "                    timestamps.append(df.index[i])\n",
    "                \n",
    "                continue\n",
    "            # print(n_steps)\n",
    "\n",
    "            # estimate next steps even if there is no measurement\n",
    "            for step in range(n_steps):\n",
    "                # Adaptive noise adjustment\n",
    "                if alpha and h > 0:\n",
    "                    # Qd = (alpha) * Qd_base + (1 - alpha) * delta_q\n",
    "                    # Rd = (alpha) * Rd_base + (1 - alpha) * delta_r\n",
    "                    Qd = (alpha) * Qd_base + (1 - alpha) * Qd_base\n",
    "                    Rd = (alpha) * Rd_base + (1 - alpha) * Rd_base\n",
    "                else:\n",
    "                    Qd, Rd = Qd_base, Rd_base\n",
    "\n",
    "                Ed = h * np.array([[0, 0],\n",
    "                                [0, 0],\n",
    "                                [1, 0],\n",
    "                                [0, 0],\n",
    "                                [0, 1]])\n",
    "\n",
    "                Rn = a / np.sqrt(1 - e**2 * np.sin(x_hat[0])**2)\n",
    "                Rm = Rn * ((1 - e**2) / (1 - e**2 * np.sin(x_hat[0])**2))\n",
    "\n",
    "                f = np.array([\n",
    "                    (1 / Rm) * x_hat[2] * np.cos(x_hat[3]) * h,\n",
    "                    (1 / (Rn * np.cos(x_hat[0]))) * x_hat[2] * np.sin(x_hat[3]) * h,\n",
    "                    -alpha_1 * x_hat[2],\n",
    "                    x_hat[4] * h,\n",
    "                    -alpha_2 * x_hat[4]\n",
    "                ])\n",
    "\n",
    "                A21 = (x_hat[2] * np.sin(x_hat[3]) * np.tan(x_hat[0])) / (Rn * np.cos(x_hat[0]))\n",
    "                Ad = np.eye(5) + h * np.array([\n",
    "                    [0, 0, (1 / Rm) * np.cos(x_hat[3]), -(1 / Rm) * x_hat[2] * np.sin(x_hat[3]), 0],\n",
    "                    [A21, 0, (1 / (Rn * np.cos(x_hat[0]))) * np.sin(x_hat[3]), (1 / (Rn * np.cos(x_hat[0]))) * x_hat[2] * np.cos(x_hat[3]), 0],\n",
    "                    [0, 0, -alpha_1, 0, 0],\n",
    "                    [0, 0, 0, 0, 1],\n",
    "                    [0, 0, 0, 0, -alpha_2]\n",
    "                ])\n",
    "\n",
    "                # print(f)\n",
    "                x_prd = x_hat + f\n",
    "                P_hat = Ad @ P_prd @ Ad.T + Ed @ Qd @ Ed.T\n",
    "\n",
    "                if step == n_steps - 1:\n",
    "                    # if there is a corresponding measurement\n",
    "                    d = ssa(y - Cd @ x_prd)\n",
    "                    S = Cd @ P_hat @ Cd.T + Rd\n",
    "                    K = P_hat @ Cd.T @ np.linalg.inv(S)\n",
    "\n",
    "                    x_hat = x_prd + K @ d\n",
    "                    x_hat[:2] = ssa(x_hat[:2])\n",
    "                    IKC = np.eye(5) - K @ Cd\n",
    "                    P_prd = IKC @ P_hat @ IKC.T + K @ Rd @ K.T\n",
    "                    \n",
    "                    # delta_q = (Cd @ K @ np.expand_dims(d, 1) @ np.expand_dims(d, 1).T @ K.T @ Cd.T) * np.eye(2) % 1e8\n",
    "            \n",
    "                    # esp = y - Cd @ x_hat\n",
    "                    # esp = np.expand_dims(esp, 1)\n",
    "                    # delta_r = (esp @ esp.T + Cd @ P_hat @ Cd.T) *np.eye(2) / R11\n",
    "                else:\n",
    "                    pass\n",
    "                    # x_hat = x_prd\n",
    "                    # x_hat[:2] = ssa(x_hat[:2])\n",
    "                    \n",
    "                # Save result\n",
    "                coord_ls.append(np.rad2deg(x_hat[:2]))\n",
    "                sog.append(x_hat[2] / (0.51444444 * (h if h > 0 else 1)))  # m/s to knots\n",
    "                cog.append(np.rad2deg(x_hat[3]) % 360)\n",
    "                if len(timestamps) == 0:\n",
    "                    timestamps.append(df.index[0])\n",
    "                else:\n",
    "                    timestamps.append(timestamps[-1] + pd.Timedelta(minutes=dt_min/60))\n",
    "\n",
    "        lat_deg, lon_deg = zip(*coord_ls)\n",
    "        df_corr = pd.DataFrame({cols.Latitude: lat_deg, cols.Longitude: lon_deg, cols.SOG: sog, cols.COG: cog}, index=timestamps)\n",
    "        # df_geo = gpd.GeoDataFrame(df_corr, geometry=gpd.points_from_xy(df_corr.longitude, df_corr.latitude), crs=CRS_METRIC)\n",
    "        # df_corr_mdp = mpd.Trajectory(df_geo, traj_id=mmsi)\n",
    "        return df_corr\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in AEKF_traj: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5cddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_original.copy()\n",
    "\n",
    "# df['target_id'] = mmsi  \n",
    "# df = df.set_index(cols.Sampled_Date)\n",
    "\n",
    "# dt_sec = np.diff(df.index.view(np.int64) // 1_000_000_000)\n",
    "# dt_min = dt_sec.min()\n",
    "# dts = np.insert(dt_sec, 0, 0)\n",
    "# # print(dts)\n",
    "\n",
    "# corrected_df = AEKF_traj_advanced(df, alpha=0.9, mmsi_colName=\"target_id\")\n",
    "# # corrected_df\n",
    "\n",
    "# df_sequences = get_trajectory_sequences(df)\n",
    "# corrected_df_sequences = get_trajectory_sequences(corrected_df)\n",
    "\n",
    "# fig = plot_plotly_trajectory_groups([corrected_df_sequences, df_sequences], group_names=[\"AEKF\", \"Initial trajectory\"])\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "# gdf = gpd.GeoDataFrame(\n",
    "#     df,\n",
    "#     geometry=gpd.points_from_xy(df[cols.Longitude], df[cols.Latitude']),\n",
    "#     crs=\"EPSG:4326\"  # WGS84, standard for lat/lon\n",
    "# )\n",
    "\n",
    "# traj = mpd.Trajectory(gdf, traj_id=mmsi, t='index')\n",
    "\n",
    "# gdf[cols.Latitude] = pd.to_numeric(gdf[cols.Latitude], errors='coerce')\n",
    "# gdf[cols.Longitude] = pd.to_numeric(gdf[cols.Longitude], errors='coerce')\n",
    "# gdf[cols.SOG] = pd.to_numeric(gdf[cols.SOG], errors='coerce')\n",
    "# gdf[cols.COG] = pd.to_numeric(gdf[cols.COG], errors='coerce')\n",
    "\n",
    "\n",
    "# corrected_traj = AEKF_traj_advanced(traj, alpha=0.9, mmsi_colName=\"target_id\")\n",
    "\n",
    "# import movingpandas as mpd\n",
    "# import hvplot.pandas  # Ensure hvplot is installed\n",
    "\n",
    "# # Interactive plot with hvplot\n",
    "# plot = traj.hvplot(\n",
    "#     geo=True,\n",
    "#     tiles=\"OSM\",  # Use OpenStreetMap as the basemap\n",
    "#     c=cols.SOG,      # Color by speed over ground\n",
    "#     cmap='Viridis',  # Color map\n",
    "#     line_width=2,\n",
    "#     title=f\"Trajectory for Vessel {traj.id}\",\n",
    "#     colorbar=True\n",
    "# )\n",
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c17851",
   "metadata": {},
   "source": [
    "## NEW AEKF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveExtendedKalmanFilter:\n",
    "    \"\"\"\n",
    "    Adaptive Extended Kalman Filter for ship trajectory estimation\n",
    "    \n",
    "    State vector: [lat, lon, sog, cog, acc, omega]\n",
    "    - lat: latitude (degrees)\n",
    "    - lon: longitude (degrees) \n",
    "    - sog: speed over ground (knots)\n",
    "    - cog: course over ground (degrees)\n",
    "    - acc: acceleration (knots/s)\n",
    "    - omega: turn rate (degrees/s)\n",
    "    \n",
    "    Measurement vector: [lat, lon, sog, cog]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dt: float = 600.0, alpha_1: float = 0.000, alpha_2: float = 0.000):\n",
    "        \"\"\"\n",
    "        Initialize AEKF\n",
    "        \n",
    "        Args:\n",
    "            dt: Time step in seconds (default 600s = 10 min)\n",
    "            alpha_1: Acceleration damping coefficient\n",
    "            alpha_2: Turn rate damping coefficient\n",
    "        \"\"\"\n",
    "        self.dt = dt\n",
    "        self.alpha_1 = alpha_1\n",
    "        self.alpha_2 = alpha_2\n",
    "        \n",
    "        # WGS-84 ellipsoid constants\n",
    "        self.a = 6378137.0  # Semi-major axis (m)\n",
    "        self.f = 1.0 / 298.257223563  # Flattening\n",
    "        self.e = np.sqrt(2 * self.f - self.f**2)  # Eccentricity\n",
    "        \n",
    "        # State dimension\n",
    "        self.n_states = 6\n",
    "        self.n_measurements = 4\n",
    "        \n",
    "        # Initialize matrices\n",
    "        self.x = np.zeros(self.n_states)  # State vector\n",
    "        self.P = np.eye(self.n_states) * 100  # Covariance matrix\n",
    "        \n",
    "        # Process noise covariance (Q) - tunable parameters\n",
    "        self.Q = np.diag([\n",
    "            1e-8,   # lat process noise (degrees²)\n",
    "            1e-8,   # lon process noise (degrees²)\n",
    "            0.01,   # sog process noise (knots²)\n",
    "            0.1,    # cog process noise (degrees²)\n",
    "            0.1,    # acc process noise (knots²/s²)\n",
    "            0.01    # omega process noise (degrees²/s²)\n",
    "        ])\n",
    "        \n",
    "        # Initial measurement noise covariance (R) - will be adapted\n",
    "        self.R = np.diag([\n",
    "            1e-6,   # lat measurement noise (degrees²)\n",
    "            1e-6,   # lon measurement noise (degrees²)\n",
    "            0.25,   # sog measurement noise (knots²)\n",
    "            1.0     # cog measurement noise (degrees²)\n",
    "        ])\n",
    "        \n",
    "        # Adaptive parameters\n",
    "        self.innovation_history = []\n",
    "        self.max_history = 10\n",
    "        self.adaptation_factor = 1.0 #0.95\n",
    "        \n",
    "    def geodetic_radii(self, lat_deg: float) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Calculate meridional and prime vertical radii of curvature\n",
    "        \n",
    "        Args:\n",
    "            lat_deg: Latitude in degrees\n",
    "            \n",
    "        Returns:\n",
    "            Rm: Meridional radius of curvature (m)\n",
    "            Rn: Prime vertical radius of curvature (m)\n",
    "        \"\"\"\n",
    "        lat_rad = np.deg2rad(lat_deg)\n",
    "        sin_lat = np.sin(lat_rad)\n",
    "        \n",
    "        # Prime vertical radius of curvature\n",
    "        Rn = self.a / np.sqrt(1 - self.e**2 * sin_lat**2)\n",
    "        \n",
    "        # Meridional radius of curvature\n",
    "        Rm = Rn * (1 - self.e**2) / (1 - self.e**2 * sin_lat**2)\n",
    "        \n",
    "        return Rm, Rn\n",
    "    \n",
    "    def state_transition(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        State transition function f(x)\n",
    "        \n",
    "        Args:\n",
    "            x: State vector [lat, lon, sog, cog, acc, omega]\n",
    "            \n",
    "        Returns:\n",
    "            Next state vector\n",
    "        \"\"\"\n",
    "        lat, lon, sog, cog, acc, omega = x\n",
    "        \n",
    "        # Convert angles to radians for calculations\n",
    "        lat_rad = np.deg2rad(lat)\n",
    "        cog_rad = np.deg2rad(cog)\n",
    "        \n",
    "        # Calculate radii of curvature\n",
    "        Rm, Rn = self.geodetic_radii(lat)\n",
    "        \n",
    "        # Convert SOG from knots to m/s for calculations\n",
    "        sog_ms = sog * 0.514444\n",
    "        \n",
    "        # State transition equations\n",
    "        x_new = np.zeros_like(x)\n",
    "        \n",
    "        # Position updates (convert back to degrees)\n",
    "        x_new[0] = lat + self.dt * np.rad2deg(sog_ms * np.cos(cog_rad) / Rm)\n",
    "        x_new[1] = lon + self.dt * np.rad2deg(sog_ms * np.sin(cog_rad) / (Rn * np.cos(lat_rad)))\n",
    "        \n",
    "        # Speed and course updates\n",
    "        x_new[2] = sog + self.dt * acc  # SOG in knots\n",
    "        x_new[3] = cog + self.dt * omega  # COG in degrees\n",
    "        \n",
    "        # Acceleration and turn rate updates (damping)\n",
    "        x_new[4] = acc * (1 - self.alpha_1)\n",
    "        x_new[5] = omega * (1 - self.alpha_2)\n",
    "        \n",
    "        # Normalize COG to [0, 360)\n",
    "        x_new[3] = x_new[3] % 360\n",
    "        \n",
    "        return x_new\n",
    "    \n",
    "    def jacobian_f(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Jacobian of state transition function\n",
    "        \n",
    "        Args:\n",
    "            x: State vector\n",
    "            \n",
    "        Returns:\n",
    "            Jacobian matrix F\n",
    "        \"\"\"\n",
    "        lat, lon, sog, cog, acc, omega = x\n",
    "        \n",
    "        lat_rad = np.deg2rad(lat)\n",
    "        cog_rad = np.deg2rad(cog)\n",
    "        \n",
    "        Rm, Rn = self.geodetic_radii(lat)\n",
    "        sog_ms = sog * 0.514444\n",
    "        \n",
    "        cos_lat = np.cos(lat_rad)\n",
    "        sin_lat = np.sin(lat_rad)\n",
    "        cos_cog = np.cos(cog_rad)\n",
    "        sin_cog = np.sin(cog_rad)\n",
    "        \n",
    "        F = np.eye(self.n_states)\n",
    "        \n",
    "        # Partial derivatives for latitude equation\n",
    "        # ∂lat_new/∂lat (includes curvature dependency)\n",
    "        dRm_dlat = self.a * (1 - self.e**2) * 2 * self.e**2 * sin_lat * cos_lat / \\\n",
    "                   ((1 - self.e**2 * sin_lat**2)**1.5 * (1 - self.e**2 * sin_lat**2))\n",
    "        F[0, 0] = 1 - self.dt * np.rad2deg(sog_ms * cos_cog * dRm_dlat / (Rm**2))\n",
    "        \n",
    "        # ∂lat_new/∂sog\n",
    "        F[0, 2] = self.dt * np.rad2deg(0.514444 * cos_cog / Rm)\n",
    "        \n",
    "        # ∂lat_new/∂cog\n",
    "        F[0, 3] = -self.dt * np.rad2deg(sog_ms * sin_cog / Rm) * np.pi/180\n",
    "        \n",
    "        # Partial derivatives for longitude equation\n",
    "        # ∂lon_new/∂lat\n",
    "        dRn_dlat = -self.a * self.e**2 * sin_lat * cos_lat / (1 - self.e**2 * sin_lat**2)**1.5\n",
    "        F[1, 0] = self.dt * np.rad2deg(sog_ms * sin_cog * sin_lat / (Rn * cos_lat**2)) * np.pi/180 - \\\n",
    "                  self.dt * np.rad2deg(sog_ms * sin_cog * dRn_dlat / (Rn**2 * cos_lat)) * np.pi/180\n",
    "        \n",
    "        # ∂lon_new/∂sog\n",
    "        F[1, 2] = self.dt * np.rad2deg(0.514444 * sin_cog / (Rn * cos_lat))\n",
    "        \n",
    "        # ∂lon_new/∂cog\n",
    "        F[1, 3] = self.dt * np.rad2deg(sog_ms * cos_cog / (Rn * cos_lat)) * np.pi/180\n",
    "        \n",
    "        # SOG equation partials\n",
    "        F[2, 2] = 1  # ∂sog_new/∂sog\n",
    "        F[2, 4] = self.dt  # ∂sog_new/∂acc\n",
    "        \n",
    "        # COG equation partials\n",
    "        F[3, 3] = 1  # ∂cog_new/∂cog\n",
    "        F[3, 5] = self.dt  # ∂cog_new/∂omega\n",
    "        \n",
    "        # Acceleration damping\n",
    "        F[4, 4] = 1 - self.alpha_1 * self.dt\n",
    "        \n",
    "        # Turn rate damping\n",
    "        F[5, 5] = 1 - self.alpha_2 * self.dt\n",
    "        \n",
    "        return F\n",
    "    \n",
    "    def measurement_function(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Measurement function h(x)\n",
    "        \n",
    "        Args:\n",
    "            x: State vector\n",
    "            \n",
    "        Returns:\n",
    "            Predicted measurement vector\n",
    "        \"\"\"\n",
    "        return x[:4]  # Measure [lat, lon, sog, cog]\n",
    "    \n",
    "    def jacobian_h(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Jacobian of measurement function\n",
    "        \n",
    "        Args:\n",
    "            x: State vector\n",
    "            \n",
    "        Returns:\n",
    "            Jacobian matrix H\n",
    "        \"\"\"\n",
    "        H = np.zeros((self.n_measurements, self.n_states))\n",
    "        H[:4, :4] = np.eye(4)  # Direct measurement of first 4 states\n",
    "        return H\n",
    "    \n",
    "    def adapt_noise_covariance(self, innovation: np.ndarray, S: np.ndarray):\n",
    "        \"\"\"\n",
    "        Adapt measurement noise covariance based on innovation\n",
    "        \n",
    "        Args:\n",
    "            innovation: Innovation vector\n",
    "            S: Innovation covariance matrix\n",
    "        \"\"\"\n",
    "        # Store innovation for adaptation\n",
    "        self.innovation_history.append(innovation)\n",
    "        if len(self.innovation_history) > self.max_history:\n",
    "            self.innovation_history.pop(0)\n",
    "        \n",
    "        if len(self.innovation_history) >= 3:\n",
    "            # Calculate sample covariance of innovations\n",
    "            innovations = np.array(self.innovation_history)\n",
    "            sample_cov = np.cov(innovations.T)\n",
    "            \n",
    "            # Adaptive update of R\n",
    "            self.R = self.adaptation_factor * self.R + \\\n",
    "                     (1 - self.adaptation_factor) * (sample_cov - \n",
    "                     self.jacobian_h(self.x) @ self.P @ self.jacobian_h(self.x).T)\n",
    "            \n",
    "            # Ensure R remains positive definite\n",
    "            try:\n",
    "                cholesky(self.R)\n",
    "            except np.linalg.LinAlgError:\n",
    "                self.R = self.R + np.eye(self.n_measurements) * 1e-6\n",
    "    \n",
    "    def predict(self):\n",
    "        \"\"\"Prediction step\"\"\"\n",
    "        # State prediction\n",
    "        self.x = self.state_transition(self.x)\n",
    "        \n",
    "        # Covariance prediction\n",
    "        F = self.jacobian_f(self.x)\n",
    "        self.P = F @ self.P @ F.T + self.Q\n",
    "    \n",
    "    def update(self, z: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update step\n",
    "        \n",
    "        Args:\n",
    "            z: Measurement vector [lat, lon, sog, cog]\n",
    "        \"\"\"\n",
    "        # Predicted measurement\n",
    "        h = self.measurement_function(self.x)\n",
    "        \n",
    "        # Innovation\n",
    "        y = z - h\n",
    "        \n",
    "        # Handle COG wraparound in innovation\n",
    "        if y[3] > 180:\n",
    "            y[3] -= 360\n",
    "        elif y[3] < -180:\n",
    "            y[3] += 360\n",
    "        \n",
    "        # Measurement Jacobian\n",
    "        H = self.jacobian_h(self.x)\n",
    "        \n",
    "        # Innovation covariance\n",
    "        S = H @ self.P @ H.T + self.R\n",
    "        \n",
    "        # Adapt noise covariance\n",
    "        self.adapt_noise_covariance(y, S)\n",
    "        \n",
    "        # Kalman gain\n",
    "        K = self.P @ H.T @ inv(S)\n",
    "        \n",
    "        # State update\n",
    "        self.x = self.x + K @ y\n",
    "        \n",
    "        # Covariance update (Joseph form for numerical stability)\n",
    "        I = np.eye(self.n_states)\n",
    "        IKH = I - K @ H\n",
    "        self.P = IKH @ self.P @ IKH.T + K @ self.R @ K.T\n",
    "        \n",
    "        # Normalize COG\n",
    "        self.x[3] = self.x[3] % 360\n",
    "    \n",
    "    def initialize(self, z0: np.ndarray, dt: float):\n",
    "        \"\"\"\n",
    "        Initialize filter with first measurement\n",
    "        \n",
    "        Args:\n",
    "            z0: Initial measurement [lat, lon, sog, cog]\n",
    "            dt: Time step\n",
    "        \"\"\"\n",
    "        self.dt = dt\n",
    "        self.x[:4] = z0\n",
    "        self.x[4:] = 0  # Initialize acc and omega to zero\n",
    "        \n",
    "        # Initial covariance - higher uncertainty for unobserved states\n",
    "        self.P = np.diag([1e-6, 1e-6, 0.25, 1.0, 1.0, 0.1])\n",
    "\n",
    "def process_trajectory_data(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Process trajectory DataFrame for AEKF\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns [latitude, longitude, sog, cog] and timestamp index\n",
    "        \n",
    "    Returns:\n",
    "        measurements: Array of measurements\n",
    "        time_steps: Array of time steps in seconds\n",
    "    \"\"\"\n",
    "    # Calculate time steps\n",
    "    timestamps = df.index\n",
    "    time_steps = np.array([(timestamps[i] - timestamps[i-1]).total_seconds() \n",
    "                          for i in range(1, len(timestamps))])\n",
    "    \n",
    "    # Extract measurements\n",
    "    measurements = df[[cols.Latitude, cols.Longitude, cols.SOG, cols.COG]].values\n",
    "    \n",
    "    return measurements, time_steps\n",
    "\n",
    "# Example usage\n",
    "def run_aekf_example(df: pd.DataFrame, max_to_predict: int = 10):\n",
    "    \"\"\"Example of running AEKF on sample data\"\"\"   \n",
    "    # Process data\n",
    "    measurements, time_steps = process_trajectory_data(df)\n",
    "    \n",
    "    # Initialize AEKF\n",
    "    aekf = AdaptiveExtendedKalmanFilter()\n",
    "    \n",
    "    # Initialize with first measurement\n",
    "    aekf.dt = time_steps.min()\n",
    "    aekf.initialize(measurements[0], aekf.dt)\n",
    "    \n",
    "    # Process measurements\n",
    "    states = [aekf.x.copy()]\n",
    "    timestamps = [df.index[0]]\n",
    "    covariances = [np.diag(aekf.P).copy()]\n",
    "    \n",
    "    for i in range(1, len(measurements)):\n",
    "        # Set time step\n",
    "        # aekf.dt = time_steps[i-1]\n",
    "        n_steps = int(time_steps[i-1]/aekf.dt)\n",
    "        \n",
    "        if n_steps > max_to_predict:\n",
    "            aekf.initialize(measurements[i], aekf.dt)\n",
    "            states.append(aekf.x.copy())\n",
    "            timestamps.append(df.index[i])\n",
    "            continue\n",
    "            \n",
    "        for step in range(n_steps):\n",
    "            # Predict\n",
    "            aekf.predict()\n",
    "            if step != n_steps - 1:\n",
    "                states.append(aekf.x.copy())\n",
    "                timestamps.append(timestamps[-1] + pd.Timedelta(minutes=aekf.dt/60))\n",
    "        \n",
    "        # Update with measurement\n",
    "        aekf.update(measurements[i])\n",
    "        \n",
    "        # Store results\n",
    "        states.append(aekf.x.copy())\n",
    "        covariances.append(np.diag(aekf.P).copy())\n",
    "        timestamps.append(timestamps[-1] + pd.Timedelta(minutes=aekf.dt/60))\n",
    "    \n",
    "    states = np.array(states)\n",
    "    covariances = np.array(covariances)\n",
    "    \n",
    "    # Print results\n",
    "    state_names = [cols.Latitude, cols.Longitude, cols.SOG, cols.COG, 'Acceleration', 'Turn Rate']\n",
    "    units = ['deg', 'deg', 'knots', 'deg', 'knots/s', 'deg/s']\n",
    "    \n",
    "    print(\"AEKF Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, (name, unit) in enumerate(zip(state_names, units)):\n",
    "        print(f\"{name:12} | Final: {states[-1, i]:10.6f} {unit:8} | \"\n",
    "              f\"Std: {np.sqrt(covariances[-1, i]):8.6f}\")\n",
    "    \n",
    "    states_df = pd.DataFrame(states, columns=state_names)\n",
    "    states_df.index = timestamps\n",
    "    \n",
    "    return states_df, covariances, df\n",
    "\n",
    "def RMSE_error_new(collection, new_collections_aekf, mask_traj):\n",
    "    \"\"\"\n",
    "    Calculate RMSE between true and filtered trajectories.\n",
    "    \"\"\"\n",
    "    RMSE_aekf = []\n",
    "    collection = [collection[i] for i, keep in enumerate(mask_traj) if keep]\n",
    "\n",
    "    for true_traj, est_traj in tqdm(zip(collection, new_collections_aekf), desc=\"RMSE\"):\n",
    "        true_coord = np.deg2rad(true_traj.df[[cols.Latitude, cols.Longitude]].values)\n",
    "        pred_coord = np.deg2rad(est_traj.df[[cols.Latitude, cols.Longitude]].values)\n",
    "        dist_matrix = haversine_distances(true_coord, pred_coord) * 6371  # km\n",
    "        diag_dist = np.diag(dist_matrix)\n",
    "        rmse = np.sqrt((diag_dist**2).mean())\n",
    "        RMSE_aekf.append(rmse)\n",
    "\n",
    "    return RMSE_aekf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb7e3b",
   "metadata": {},
   "source": [
    "## Old Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e857254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def fit_circular_distributions(angles):\n",
    "#     \"\"\"\n",
    "#     Takes a pandas Series of angles (degrees, -180 to 180) and evaluates the likelihood\n",
    "#     of various circular distributions (von Mises, wrapped normal, wrapped Cauchy, uniform).\n",
    "    \n",
    "#     Parameters:\n",
    "#     angles (pd.Series): Series of angles in degrees (-180 to 180).\n",
    "    \n",
    "#     Returns:\n",
    "#     dict: Dictionary with distribution names, log-likelihoods, and fitted parameters.\n",
    "#     \"\"\"\n",
    "#     # Convert angles to radians\n",
    "#     angles_rad = np.deg2rad(angles)\n",
    "    \n",
    "#     # angles are in [-pi, pi)\n",
    "#     # angles_rad = np.mod(angles_rad, np.pi)\n",
    "    \n",
    "#     # Initialize results dictionary\n",
    "#     results = {}\n",
    "    \n",
    "#     # 1. Von Mises Distribution\n",
    "#     def neg_log_likelihood_vonmises(params, data):\n",
    "#         mu, kappa = params\n",
    "#         return -np.sum(vonmises.logpdf(data, kappa=kappa, loc=mu))\n",
    "    \n",
    "#     # Initial guess: mean angle and kappa=1\n",
    "#     mu_init = np.angle(np.mean(np.exp(1j * angles_rad)))  # Circular mean\n",
    "#     kappa_init = 1.0\n",
    "#     res_vonmises = minimize(\n",
    "#         neg_log_likelihood_vonmises,\n",
    "#         x0=[mu_init, kappa_init],\n",
    "#         args=(angles_rad,),\n",
    "#         bounds=[(-np.pi, np.pi), (0, 100)]  # kappa > 0, mu in [-pi, pi]\n",
    "#     )\n",
    "#     results['von_mises'] = {\n",
    "#         'log_likelihood': -res_vonmises.fun,\n",
    "#         'parameters': {'mu': res_vonmises.x[0], 'kappa': res_vonmises.x[1]}\n",
    "#     }\n",
    "    \n",
    "#     # 2. Wrapped Normal Distribution\n",
    "#     def wrapped_normal_logpdf(data, mu, sigma):\n",
    "#         # Approximate wrapped normal by summing over a few wraps\n",
    "#         n = 10  # Number of wraps to consider\n",
    "#         logpdf = np.zeros_like(data)\n",
    "#         for k in range(-n, n+1):\n",
    "#             logpdf += norm.pdf(data + 2 * np.pi * k, loc=mu, scale=sigma)\n",
    "#         return np.log(np.maximum(logpdf, 1e-10))  # Avoid log(0)\n",
    "    \n",
    "#     def neg_log_likelihood_wrapped_normal(params, data):\n",
    "#         mu, sigma = params\n",
    "#         return -np.sum(wrapped_normal_logpdf(data, mu, sigma))\n",
    "    \n",
    "#     sigma_init = np.std(angles_rad)  # Initial guess for sigma\n",
    "#     res_wrapped_normal = minimize(\n",
    "#         neg_log_likelihood_wrapped_normal,\n",
    "#         x0=[mu_init, sigma_init],\n",
    "#         args=(angles_rad,),\n",
    "#         bounds=[(-np.pi, np.pi), (0.01, 10)]  # sigma > 0\n",
    "#     )\n",
    "#     results['wrapped_normal'] = {\n",
    "#         'log_likelihood': -res_wrapped_normal.fun,\n",
    "#         'parameters': {'mu': res_wrapped_normal.x[0], 'sigma': res_wrapped_normal.x[1]}\n",
    "#     }\n",
    "    \n",
    "#     # 3. Wrapped Cauchy Distribution\n",
    "#     def wrapped_cauchy_logpdf(data, mu, gamma):\n",
    "#         # Approximate wrapped Cauchy\n",
    "#         n = 10\n",
    "#         logpdf = np.zeros_like(data)\n",
    "#         for k in range(-n, n+1):\n",
    "#             logpdf += cauchy.pdf(data + 2 * np.pi * k, loc=mu, scale=gamma)\n",
    "#         return np.log(np.maximum(logpdf, 1e-10))\n",
    "    \n",
    "#     def neg_log_likelihood_wrapped_cauchy(params, data):\n",
    "#         mu, gamma = params\n",
    "#         return -np.sum(wrapped_cauchy_logpdf(data, mu, gamma))\n",
    "    \n",
    "#     gamma_init = 0.1  # Initial guess for scale\n",
    "#     res_wrapped_cauchy = minimize(\n",
    "#         neg_log_likelihood_wrapped_cauchy,\n",
    "#         x0=[mu_init, gamma_init],\n",
    "#         args=(angles_rad,),\n",
    "#         bounds=[(-np.pi, np.pi), (0.01, 5)]  # gamma > 0\n",
    "#     )\n",
    "#     results['wrapped_cauchy'] = {\n",
    "#         'log_likelihood': -res_wrapped_cauchy.fun,\n",
    "#         'parameters': {'mu': res_wrapped_cauchy.x[0], 'gamma': res_wrapped_cauchy.x[1]}\n",
    "#     }\n",
    "    \n",
    "#     # 4. Uniform Circular Distribution\n",
    "#     # Log-likelihood for uniform is constant: log(1/(2pi)) per sample\n",
    "#     uniform_logpdf = -np.log(2 * np.pi)\n",
    "#     results['uniform'] = {\n",
    "#         'log_likelihood': uniform_logpdf * len(angles_rad),\n",
    "#         'parameters': {}  # No parameters to estimate\n",
    "#     }\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def wrapped_normal_pdf(x, mu, sigma, terms=5):\n",
    "#     \"\"\"\n",
    "#     Evaluate the wrapped normal PDF at angle x.\n",
    "#     Parameters:\n",
    "#         x     : array of angles in radians (0 to 2π)\n",
    "#         mu    : mean direction\n",
    "#         sigma : standard deviation of underlying normal\n",
    "#         terms : number of terms to sum on each side\n",
    "#     \"\"\"\n",
    "#     k = np.arange(-terms, terms+1)\n",
    "#     x = np.atleast_1d(x)\n",
    "#     shifted = x[:, None] - mu + 2 * np.pi * k  # shape (N, 2*terms+1)\n",
    "#     gaussians = np.exp(-0.5 * (shifted / sigma)**2)\n",
    "#     norm = sigma * np.sqrt(2 * np.pi)\n",
    "#     return np.sum(gaussians, axis=1) / norm\n",
    "\n",
    "# def neg_log_likelihood_wrapped_normal(params, data):\n",
    "#     mu, log_sigma = params\n",
    "#     sigma = np.exp(log_sigma)  # optimize log(sigma) for stability\n",
    "#     pdf_vals = wrapped_normal_pdf(data, mu, sigma, terms=5)\n",
    "#     pdf_vals = np.clip(pdf_vals, 1e-12, None)  # prevent log(0)\n",
    "#     return -np.sum(np.log(pdf_vals))\n",
    "\n",
    "# def fit_wrapped_normal(data):\n",
    "#     \"\"\"\n",
    "#     Fit wrapped normal using MLE.\n",
    "#     Returns: mu, sigma\n",
    "#     \"\"\"\n",
    "#     data = np.mod(data, 2*np.pi)  # ensure [0, 2π]\n",
    "#     mu_init = np.angle(np.mean(np.exp(1j * data)))\n",
    "#     sigma_init = np.std(data)\n",
    "#     res = minimize(neg_log_likelihood_wrapped_normal, \n",
    "#                    x0=[mu_init, np.log(sigma_init)],\n",
    "#                    args=(data,),\n",
    "#                    bounds=[(0, 2*np.pi), (np.log(1e-3), np.log(10))])\n",
    "#     mu, log_sigma = res.x\n",
    "#     sigma = np.exp(log_sigma)\n",
    "#     log_lik = -res.fun\n",
    "#     return mu, sigma, log_lik\n",
    "\n",
    "# def evaluate_circular_distributions(angle_series: pd.Series) -> pd.DataFrame:\n",
    "#     # Convert to radians\n",
    "#     data = np.deg2rad(angle_series.dropna().values)\n",
    "#     print(data)\n",
    "#     data = np.mod(data, 2*np.pi)\n",
    "#     print(data)\n",
    "    \n",
    "#     # Define distributions\n",
    "#     results = []\n",
    "    \n",
    "#     # 1. Von Mises\n",
    "#     kappa, loc, scale = vonmises.fit(data, fscale=1)  # scale fixed to 1 for circular data\n",
    "#     log_lik = np.sum(vonmises.logpdf(data, kappa, loc=loc))\n",
    "#     aic = 2 * 2 - 2 * log_lik\n",
    "#     results.append(('vonmises', log_lik, aic))\n",
    "    \n",
    "#     # 2. Wrapped Cauchy\n",
    "#     fit_params = wrapcauchy.fit(data, floc=0, fscale=2*np.pi)\n",
    "#     c, loc, scale = fit_params\n",
    "#     log_lik = np.sum(wrapcauchy.logpdf(data, c, loc=loc, scale=scale))\n",
    "#     aic = 2 * 1 - 2 * log_lik  # 1 parameter estimated: c\n",
    "#     results.append(('wrapcauchy', log_lik, aic))\n",
    "    \n",
    "#     # 3. Uniform\n",
    "#     log_lik = np.sum(uniform.logpdf(data, loc=0, scale=2*np.pi))\n",
    "#     aic = 2 * 0 - 2 * log_lik\n",
    "#     results.append(('uniform', log_lik, aic))\n",
    "    \n",
    "\n",
    "#     # 3.  Wrapped Normal\n",
    "#     mu, sigma, log_lik = fit_wrapped_normal(data)\n",
    "#     aic = 2 * 2 - 2 * log_lik  # 2 parameters: mu, sigma\n",
    "#     results.append(('wrapped_normal', log_lik, aic))\n",
    "\n",
    "#     # Normalize AIC scores to get probabilities\n",
    "#     aics = np.array([r[2] for r in results])\n",
    "#     delta_aic = aics - np.min(aics)\n",
    "#     rel_likelihoods = np.exp(-0.5 * delta_aic)\n",
    "#     probabilities = rel_likelihoods / np.sum(rel_likelihoods)\n",
    "\n",
    "#     # Prepare output\n",
    "#     return pd.DataFrame({\n",
    "#         'distribution': [r[0] for r in results],\n",
    "#         'log_likelihood': [r[1] for r in results],\n",
    "#         'AIC': aics,\n",
    "#         'probability': probabilities\n",
    "#     }).sort_values(by='probability', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103711fb",
   "metadata": {},
   "source": [
    "## Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_turning_angles(df):\n",
    "    \"\"\"\n",
    "    Calculate turning angles between consecutive points in a DataFrame using latitude and longitude.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with timestamp index and 'latitude' and 'longitude' columns (in degrees)\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: Series with turning angles in degrees, indexed by the second timestamp of each pair\n",
    "    \"\"\"\n",
    "    def calculate_bearing(lat1, lon1, lat2, lon2):\n",
    "        \"\"\"\n",
    "        Calculate the bearing (azimuth) between two points in degrees.\n",
    "        \"\"\"\n",
    "        # Convert latitude and longitude to radians\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "        \n",
    "        # Difference in longitude\n",
    "        dlon = lon2 - lon1\n",
    "        \n",
    "        # Calculate bearing\n",
    "        x = np.sin(dlon) * np.cos(lat2)\n",
    "        y = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dlon)\n",
    "        bearing = np.arctan2(x, y)\n",
    "        \n",
    "        # Convert to degrees and normalize to [0, 360]\n",
    "        bearing = np.degrees(bearing)\n",
    "        bearing = (bearing + 360) % 360\n",
    "        return bearing\n",
    "    \n",
    "    # Initialize lists to store bearings and timestamps\n",
    "    bearings = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    # Iterate over consecutive pairs of points\n",
    "    for i in range(len(df) - 1):\n",
    "        lat1, lon1 = df.iloc[i]['latitude'], df.iloc[i]['longitude']\n",
    "        lat2, lon2 = df.iloc[i + 1]['latitude'], df.iloc[i + 1]['longitude']\n",
    "        bearing = calculate_bearing(lat1, lon1, lat2, lon2)\n",
    "        bearings.append(bearing)\n",
    "        valid_indices.append(df.index[i + 1])\n",
    "    \n",
    "    # Calculate turning angles as the difference between consecutive bearings\n",
    "    turning_angles = []\n",
    "    for i in range(len(bearings) - 1):\n",
    "        angle = bearings[i + 1] - bearings[i]\n",
    "        # Normalize to [-180, 180]\n",
    "        angle = ((angle + 180) % 360 - 180)\n",
    "        turning_angles.append(angle)\n",
    "    \n",
    "    # Create Series with turning angles, indexed by the second timestamp of each triplet\n",
    "    result = pd.Series(turning_angles, index=valid_indices[1:], name='turning_angle')\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def angles_rad_to_unique_bins(angles_rad, num_bins=72):\n",
    "    some_bins = np.linspace(0, 2 * np.pi, num_bins + 1)\n",
    "    some_bin_indices = np.digitize(angles_rad, some_bins, right=False)\n",
    "    some_bin_indices = np.clip(some_bin_indices - 1, 0, num_bins - 1)\n",
    "    some_bin_midpoints = (some_bins[:-1] + some_bins[1:]) / 2\n",
    "    binned_angles = some_bin_midpoints[some_bin_indices]\n",
    "    angles_rad = np.unique(binned_angles)\n",
    "    return angles_rad\n",
    "\n",
    "def plot_circular_distribution(angles, bins=36, title=\"Circular Distribution of Angles\", use_bins=True):\n",
    "    \"\"\"\n",
    "    Create a polar histogram for a pandas Series of angles (in degrees) using Plotly.\n",
    "    \n",
    "    Parameters:\n",
    "    angles (pd.Series): Series containing angles in degrees\n",
    "    bins (int): Number of bins for the histogram (default: 36)\n",
    "    title (str): Plot title (default: \"Circular Distribution of Angles\")\n",
    "    \n",
    "    Returns:\n",
    "    plotly.graph_objects.Figure: The created figure object\n",
    "    \"\"\"\n",
    "    # Convert angles to radians\n",
    "    angles_rad = np.deg2rad(angles)\n",
    "    \n",
    "    if use_bins:\n",
    "        angles_rad= angles_rad_to_unique_bins(angles_rad)\n",
    "    \n",
    "    # Create histogram\n",
    "    counts, bin_edges = np.histogram(angles_rad, bins=bins, range=(-np.pi, np.pi))\n",
    "    # Calculate bin centers in degrees for plotting\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    bin_centers_deg = np.rad2deg(bin_centers)\n",
    "    \n",
    "    # Create polar bar plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Barpolar(\n",
    "            r=counts,\n",
    "            theta=bin_centers_deg,\n",
    "            width=360/bins,  # Width of each bar in degrees\n",
    "            marker=dict(\n",
    "                color=counts,\n",
    "                colorscale='Aggrnyl',\n",
    "                showscale=True,\n",
    "                colorbar_title=\"Count\"\n",
    "            ),\n",
    "            name='Angle Distribution'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update layout for better visualization\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        polar=dict(\n",
    "            angularaxis=dict(\n",
    "                rotation=90,  # Rotate so 0° is at top\n",
    "                direction=\"clockwise\",\n",
    "                tickvals=np.linspace(0, 360, 8, endpoint=False),\n",
    "                ticktext=[f\"{int(t)}°\" for t in np.linspace(0, 360, 8, endpoint=False)]\n",
    "            ),\n",
    "            # radialaxis=dict(\n",
    "            #     visible=True,\n",
    "            #     title=\"Count\"\n",
    "            # )\n",
    "        ),\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def fit_circular_distributions(angles_series):\n",
    "    \"\"\"\n",
    "    Test a pandas Series of angles (-180 to 180) against various circular distributions\n",
    "    and return the probability/likelihood of each distribution fitting the data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    angles_series : pd.Series\n",
    "        Series of angles in degrees from -180 to 180\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with distribution names, parameters, log-likelihood, AIC, BIC, and relative probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert angles to radians and ensure they're in [0, 2π)\n",
    "    angles_rad = np.deg2rad(angles_series.dropna())\n",
    "    angles_rad = (angles_rad + 2*np.pi) % (2*np.pi)\n",
    "    \n",
    "    num_bins = 72\n",
    "    bins = np.linspace(0, 2 * np.pi, num_bins + 1)\n",
    "    bin_indices = np.digitize(angles_rad, bins, right=False)\n",
    "    bin_indices = np.clip(bin_indices - 1, 0, num_bins - 1)\n",
    "    bin_midpoints = (bins[:-1] + bins[1:]) / 2\n",
    "    binned_angles = bin_midpoints[bin_indices]\n",
    "    angles_rad = np.unique(binned_angles)\n",
    "    \n",
    "    n = len(angles_rad)\n",
    "    \n",
    "    if n < 2:\n",
    "        raise ValueError(\"Need at least 2 data points\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # 1. Uniform Circular Distribution\n",
    "    def uniform_circular_loglik(angles):\n",
    "        return n * np.log(1/(2*np.pi))\n",
    "    \n",
    "    ll_uniform = uniform_circular_loglik(angles_rad)\n",
    "    results.append({\n",
    "        'distribution': 'Uniform Circular',\n",
    "        'parameters': {},\n",
    "        'log_likelihood': ll_uniform,\n",
    "        'n_params': 0\n",
    "    })\n",
    "    \n",
    "    # 2. Von Mises Distribution\n",
    "    def vonmises_negloglik(params, angles):\n",
    "        mu, kappa = params\n",
    "        if kappa <= 0:\n",
    "            return np.inf\n",
    "        try:\n",
    "            # Von Mises log-likelihood\n",
    "            ll = n * np.log(1/(2*np.pi*stats.i0(kappa))) + kappa * np.sum(np.cos(angles - mu))\n",
    "            return -ll\n",
    "        except:\n",
    "            return np.inf\n",
    "    \n",
    "    # Fit Von Mises\n",
    "    try:\n",
    "        # Initial estimates\n",
    "        mean_angle = np.arctan2(np.mean(np.sin(angles_rad)), np.mean(np.cos(angles_rad)))\n",
    "        R = np.sqrt(np.mean(np.cos(angles_rad))**2 + np.mean(np.sin(angles_rad))**2)\n",
    "        kappa_init = max(0.1, 2*R/(1-R**2)) if R < 0.95 else 10\n",
    "        \n",
    "        res = minimize(vonmises_negloglik, [mean_angle, kappa_init], \n",
    "                      args=(angles_rad,), method='L-BFGS-B',\n",
    "                      bounds=[(-np.pi, np.pi), (0.01, 100)])\n",
    "        \n",
    "        if res.success:\n",
    "            mu_fit, kappa_fit = res.x\n",
    "            ll_vonmises = -res.fun\n",
    "            results.append({\n",
    "                'distribution': 'Von Mises',\n",
    "                'parameters': {'mu': np.rad2deg(mu_fit), 'kappa': kappa_fit},\n",
    "                'log_likelihood': ll_vonmises,\n",
    "                'n_params': 2\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 3. Wrapped Normal Distribution\n",
    "    def wrapped_normal_negloglik(params, angles):\n",
    "        mu, sigma = params\n",
    "        if sigma <= 0:\n",
    "            return np.inf\n",
    "        try:\n",
    "            # Wrapped normal approximation using first few terms\n",
    "            ll = 0\n",
    "            for angle in angles:\n",
    "                density = 0\n",
    "                for k in range(-3, 4):  # Sum over wrapping terms\n",
    "                    density += np.exp(-((angle - mu + 2*np.pi*k)**2)/(2*sigma**2))\n",
    "                density /= (sigma * np.sqrt(2*np.pi))\n",
    "                ll += np.log(max(density, 1e-300))\n",
    "            return -ll\n",
    "        except:\n",
    "            return np.inf\n",
    "    \n",
    "    # Fit Wrapped Normal\n",
    "    try:\n",
    "        mean_angle = np.arctan2(np.mean(np.sin(angles_rad)), np.mean(np.cos(angles_rad)))\n",
    "        sigma_init = 1.0\n",
    "        \n",
    "        res = minimize(wrapped_normal_negloglik, [mean_angle, sigma_init],\n",
    "                      args=(angles_rad,), method='L-BFGS-B',\n",
    "                      bounds=[(-np.pi, np.pi), (0.1, 5)])\n",
    "        \n",
    "        if res.success:\n",
    "            mu_fit, sigma_fit = res.x\n",
    "            ll_wrapped_normal = -res.fun\n",
    "            results.append({\n",
    "                'distribution': 'Wrapped Normal',\n",
    "                'parameters': {'mu': np.rad2deg(mu_fit), 'sigma': sigma_fit},\n",
    "                'log_likelihood': ll_wrapped_normal,\n",
    "                'n_params': 2\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 4. Wrapped Gaussian Mixture (2 components)\n",
    "    def wrapped_gaussian_mixture_negloglik(params, angles):\n",
    "        if len(params) != 5:  # mu1, sigma1, mu2, sigma2, weight1\n",
    "            return np.inf\n",
    "        \n",
    "        mu1, sigma1, mu2, sigma2, w1 = params\n",
    "        w2 = 1 - w1\n",
    "        \n",
    "        if sigma1 <= 0 or sigma2 <= 0 or not (0 < w1 < 1):\n",
    "            return np.inf\n",
    "        \n",
    "        try:\n",
    "            ll = 0\n",
    "            for angle in angles:\n",
    "                # Component 1\n",
    "                density1 = 0\n",
    "                for k in range(-3, 4):\n",
    "                    density1 += np.exp(-((angle - mu1 + 2*np.pi*k)**2)/(2*sigma1**2))\n",
    "                density1 /= (sigma1 * np.sqrt(2*np.pi))\n",
    "                \n",
    "                # Component 2\n",
    "                density2 = 0\n",
    "                for k in range(-3, 4):\n",
    "                    density2 += np.exp(-((angle - mu2 + 2*np.pi*k)**2)/(2*sigma2**2))\n",
    "                density2 /= (sigma2 * np.sqrt(2*np.pi))\n",
    "                \n",
    "                # Mixture\n",
    "                mixture_density = w1 * density1 + w2 * density2\n",
    "                ll += np.log(max(mixture_density, 1e-300))\n",
    "            \n",
    "            return -ll\n",
    "        except:\n",
    "            return np.inf\n",
    "    \n",
    "    # Fit Wrapped Gaussian Mixture (2 components)\n",
    "    try:\n",
    "        # Initialize with k-means-like approach on angles\n",
    "        # Simple initialization: split data into two groups\n",
    "        sorted_angles = np.sort(angles_rad)\n",
    "        mid_idx = len(sorted_angles) // 2\n",
    "        \n",
    "        mu1_init = np.arctan2(np.mean(np.sin(sorted_angles[:mid_idx])), \n",
    "                             np.mean(np.cos(sorted_angles[:mid_idx])))\n",
    "        mu2_init = np.arctan2(np.mean(np.sin(sorted_angles[mid_idx:])), \n",
    "                             np.mean(np.cos(sorted_angles[mid_idx:])))\n",
    "        \n",
    "        # Ensure means are separated\n",
    "        if abs(mu1_init - mu2_init) < np.pi/4:\n",
    "            mu2_init = mu1_init + np.pi\n",
    "            mu2_init = (mu2_init + np.pi) % (2*np.pi) - np.pi\n",
    "        \n",
    "        sigma1_init = 1.0\n",
    "        sigma2_init = 1.0\n",
    "        w1_init = 0.5\n",
    "        \n",
    "        res = minimize(wrapped_gaussian_mixture_negloglik, \n",
    "                      [mu1_init, sigma1_init, mu2_init, sigma2_init, w1_init],\n",
    "                      args=(angles_rad,), method='L-BFGS-B',\n",
    "                      bounds=[(-np.pi, np.pi), (0.1, 5), (-np.pi, np.pi), (0.1, 5), (0.1, 0.9)])\n",
    "        \n",
    "        if res.success:\n",
    "            mu1_fit, sigma1_fit, mu2_fit, sigma2_fit, w1_fit = res.x\n",
    "            ll_wgm = -res.fun\n",
    "            results.append({\n",
    "                'distribution': 'Wrapped Gaussian Mixture (2 comp)',\n",
    "                'parameters': {\n",
    "                    'mu1': np.rad2deg(mu1_fit), 'sigma1': sigma1_fit,\n",
    "                    'mu2': np.rad2deg(mu2_fit), 'sigma2': sigma2_fit,\n",
    "                    'weight1': w1_fit, 'weight2': 1-w1_fit\n",
    "                },\n",
    "                'log_likelihood': ll_wgm,\n",
    "                'n_params': 5\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 5. Wrapped Cauchy Distribution\n",
    "    def wrapped_cauchy_negloglik(params, angles):\n",
    "        mu, rho = params\n",
    "        if not (0 < rho < 1):\n",
    "            return np.inf\n",
    "        try:\n",
    "            ll = n * np.log((1-rho**2)/(2*np.pi)) - np.sum(np.log(1 + rho**2 - 2*rho*np.cos(angles - mu)))\n",
    "            return -ll\n",
    "        except:\n",
    "            return np.inf\n",
    "    \n",
    "    # Fit Wrapped Cauchy\n",
    "    try:\n",
    "        mean_angle = np.arctan2(np.mean(np.sin(angles_rad)), np.mean(np.cos(angles_rad)))\n",
    "        rho_init = 0.5\n",
    "        \n",
    "        res = minimize(wrapped_cauchy_negloglik, [mean_angle, rho_init],\n",
    "                      args=(angles_rad,), method='L-BFGS-B',\n",
    "                      bounds=[(-np.pi, np.pi), (0.01, 0.99)])\n",
    "        \n",
    "        if res.success:\n",
    "            mu_fit, rho_fit = res.x\n",
    "            ll_wrapped_cauchy = -res.fun\n",
    "            results.append({\n",
    "                'distribution': 'Wrapped Cauchy',\n",
    "                'parameters': {'mu': np.rad2deg(mu_fit), 'rho': rho_fit},\n",
    "                'log_likelihood': ll_wrapped_cauchy,\n",
    "                'n_params': 2\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Calculate AIC, BIC, and relative probabilities\n",
    "    df = pd.DataFrame(results)\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df['AIC'] = 2 * df['n_params'] - 2 * df['log_likelihood']\n",
    "    df['BIC'] = np.log(n) * df['n_params'] - 2 * df['log_likelihood']\n",
    "    \n",
    "    # Calculate Akaike weights (relative probabilities based on AIC)\n",
    "    min_aic = df['AIC'].min()\n",
    "    df['delta_AIC'] = df['AIC'] - min_aic\n",
    "    df['akaike_weight'] = np.exp(-0.5 * df['delta_AIC'])\n",
    "    df['akaike_weight'] = df['akaike_weight'] / df['akaike_weight'].sum()\n",
    "    \n",
    "    # Calculate BIC weights\n",
    "    min_bic = df['BIC'].min()\n",
    "    df['delta_BIC'] = df['BIC'] - min_bic\n",
    "    df['bic_weight'] = np.exp(-0.5 * df['delta_BIC'])\n",
    "    df['bic_weight'] = df['bic_weight'] / df['bic_weight'].sum()\n",
    "    \n",
    "    # Sort by AIC (best fit first)\n",
    "    df = df.sort_values('AIC').reset_index(drop=True)\n",
    "    \n",
    "    # Round numerical values for display\n",
    "    df['log_likelihood'] = df['log_likelihood'].round(3)\n",
    "    df['AIC'] = df['AIC'].round(3)\n",
    "    df['BIC'] = df['BIC'].round(3)\n",
    "    df['akaike_weight'] = df['akaike_weight'].round(4)\n",
    "    df['bic_weight'] = df['bic_weight'].round(4)\n",
    "    \n",
    "    return df[['distribution', 'parameters', 'log_likelihood', 'AIC', 'BIC', \n",
    "               'akaike_weight', 'bic_weight']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5e1aba",
   "metadata": {},
   "source": [
    "## Fractal dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79301fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees) in kilometers\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    r = 6371  # Radius of earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "def calculate_trajectory_length(df):\n",
    "    \"\"\"Calculate total trajectory length using haversine distance\"\"\"\n",
    "    total_length = 0\n",
    "    for i in range(1, len(df)):\n",
    "        dist = haversine_distance(\n",
    "            df.iloc[i-1]['latitude'], df.iloc[i-1]['longitude'],\n",
    "            df.iloc[i]['latitude'], df.iloc[i]['longitude']\n",
    "        )\n",
    "        total_length += dist\n",
    "    return total_length\n",
    "\n",
    "def divider_method_fractal_dimension(df, scales=None):\n",
    "    \"\"\"\n",
    "    Calculate fractal dimension using the divider method (ruler method)\n",
    "    This method measures the trajectory length at different scales\n",
    "    \"\"\"\n",
    "    if scales is None:\n",
    "        # Generate logarithmically spaced scales\n",
    "        min_scale = 0.1  # km\n",
    "        max_scale = 10.0  # km\n",
    "        scales = np.logspace(np.log10(min_scale), np.log10(max_scale), 20)\n",
    "    \n",
    "    lengths = []\n",
    "    \n",
    "    for scale in scales:\n",
    "        # Resample trajectory at given scale intervals\n",
    "        cumulative_dist = 0\n",
    "        simplified_points = [0]  # Start with first point\n",
    "        \n",
    "        for i in range(1, len(df)):\n",
    "            dist = haversine_distance(\n",
    "                df.iloc[simplified_points[-1]]['latitude'], df.iloc[simplified_points[-1]]['longitude'],\n",
    "                df.iloc[i]['latitude'], df.iloc[i]['longitude']\n",
    "            )\n",
    "            cumulative_dist += dist\n",
    "            \n",
    "            if cumulative_dist >= scale:\n",
    "                simplified_points.append(i)\n",
    "                cumulative_dist = 0\n",
    "        \n",
    "        # Calculate length of simplified trajectory\n",
    "        length = 0.000000001\n",
    "        for i in range(1, len(simplified_points)):\n",
    "            idx1, idx2 = simplified_points[i-1], simplified_points[i]\n",
    "            dist = haversine_distance(\n",
    "                df.iloc[idx1]['latitude'], df.iloc[idx1]['longitude'],\n",
    "                df.iloc[idx2]['latitude'], df.iloc[idx2]['longitude']\n",
    "            )\n",
    "            length += dist\n",
    "        \n",
    "        lengths.append(length)\n",
    "    \n",
    "    # Fit power law: L(δ) = C * δ^(1-D)\n",
    "    # Taking log: log(L) = log(C) + (1-D) * log(δ)\n",
    "    log_scales = np.log10(scales)\n",
    "    log_lengths = np.log10(lengths)\n",
    "    \n",
    "    # Linear regression\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(log_scales.reshape(-1, 1), log_lengths)\n",
    "    slope = reg.coef_[0]\n",
    "    \n",
    "    # Fractal dimension D = 1 - slope\n",
    "    fractal_dimension = 1 - slope\n",
    "    \n",
    "    return fractal_dimension, scales, lengths, slope\n",
    "\n",
    "# Example usage with your data\n",
    "def analyze_ship_trajectory():\n",
    "    # Your trajectory data\n",
    "    data = {\n",
    "        'latitude': [33.505038, 33.506835, 33.500507, 33.503695, 33.513497],\n",
    "        'longitude': [127.450177, 127.393068, 127.332849, 127.273373, 127.213807]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(\"Ship Trajectory Fractal Dimension Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Calculate fractal dimensions using both methods\n",
    "    divider_results = divider_method_fractal_dimension(df)\n",
    "    # box_results = box_counting_fractal_dimension(df)\n",
    "    \n",
    "    d_dim, _, _, _ = divider_results\n",
    "    # b_dim, _, _, _ = box_results\n",
    "    \n",
    "    print(f\"Divider Method Fractal Dimension: {d_dim:.3f}\")\n",
    "    # print(f\"Box Counting Fractal Dimension: {b_dim:.3f}\")\n",
    "    \n",
    "    # Additional trajectory metrics\n",
    "    total_length = calculate_trajectory_length(df)\n",
    "    straight_dist = haversine_distance(\n",
    "        df.iloc[0]['latitude'], df.iloc[0]['longitude'],\n",
    "        df.iloc[-1]['latitude'], df.iloc[-1]['longitude']\n",
    "    )\n",
    "    \n",
    "    print(f\"Total trajectory length: {total_length:.2f} km\")\n",
    "    print(f\"Straight-line distance: {straight_dist:.2f} km\")\n",
    "    print(f\"Sinuosity ratio: {total_length/straight_dist:.2f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    # plot_fractal_analysis(df, divider_results, box_results)\n",
    "    \n",
    "    return df, divider_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9cbe40",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "fe9b0926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388 440716900\n"
     ]
    }
   ],
   "source": [
    "file = \"../../data/FishingKoreaAIS_sampled_new/len_388_mmsi_440716900_eta_val_2024-02-08 15:00:00_dest_JEJU.csv\"\n",
    "len_match = re.search(r'len_(\\d+)_mmsi_(\\d+)', file)\n",
    "if len_match:\n",
    "    length = int(len_match.group(1))\n",
    "    mmsi = len_match.group(2)\n",
    "    print(length, mmsi)\n",
    "df_original = pd.read_csv(file, index_col=0)\n",
    "df_original[cols.Sampled_Date] = pd.to_datetime(df_original[cols.Sampled_Date], errors=\"coerce\")\n",
    "\n",
    "df = df_original.copy()\n",
    "df['target_id'] = mmsi  \n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6381247",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_to_predict = 20\n",
    "df = df_original.copy()\n",
    "df = df.set_index(cols.Sampled_Date)\n",
    "states_df, covariances, df = run_aekf_example(df, n_to_predict)\n",
    "\n",
    "df_sequences = get_trajectory_sequences(df)\n",
    "corrected_df_sequences = get_trajectory_sequences(states_df)\n",
    "\n",
    "fig = plot_plotly_trajectory_groups([corrected_df_sequences, df_sequences], group_names=[\"AEKF\", \"Initial trajectory\"])\n",
    "fig.show()\n",
    "# fig.write_html(f\"../../results/{file.split(\"/\")[-1][:-4]}_n_to_predict_{n_to_predict}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509683f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_window = 20\n",
    "# min_window = 10\n",
    "window = 20\n",
    "step = 5\n",
    "for corrected_df_sequence in corrected_df_sequences:\n",
    "    for i in range(int(len(corrected_df_sequence) / step)):\n",
    "        current_segment = corrected_df_sequence[step*i:window+step*i]\n",
    "        if len(current_segment) < window:\n",
    "            continue\n",
    "        \n",
    "        pseudo_area = (current_segment[cols.Latitude].max() - current_segment[cols.Latitude].min()) * (current_segment[cols.Longitude].max() - current_segment[cols.Longitude].min())\n",
    "        # print(pseudo_area)\n",
    "        if pseudo_area < 0.002:\n",
    "            continue\n",
    "        \n",
    "        original_traj_segment = df[(df.index >= current_segment.index.min()) & (df.index <= current_segment.index.max())]\n",
    "        \n",
    "        # if i == 11:\n",
    "        #     print(current_segment)\n",
    "        #     original_traj_segment_sequences = get_trajectory_sequences(original_traj_segment)\n",
    "        #     fig = plot_plotly_trajectory_groups([[current_segment], original_traj_segment_sequences], group_names=[\"AEKF\", \"Initial trajectory\"])\n",
    "        #     fig.show()\n",
    "            \n",
    "        divider_results = divider_method_fractal_dimension(current_segment)\n",
    "        d_dim, _, _, _ = divider_results\n",
    "        print(f\"Divider Method Fractal Dimension for i={i}: {d_dim:.3f}\")\n",
    "\n",
    "        \n",
    "        if d_dim > 1.07 and d_dim < 2:\n",
    "            turning_angles = calculate_turning_angles(current_segment)\n",
    "            # print(turning_angles)\n",
    "            \n",
    "            results = fit_circular_distributions(turning_angles)\n",
    "            # results = evaluate_circular_distributions(turning_angles)\n",
    "            print(results)\n",
    "            \n",
    "            divider_results = divider_method_fractal_dimension(current_segment)\n",
    "            d_dim, _, _, _ = divider_results\n",
    "            print(f\"Divider Method Fractal Dimension: {d_dim:.3f}\")\n",
    "            # # Print results\n",
    "            # for dist_name, info in results.items():\n",
    "            #     print(f\"\\nDistribution: {dist_name}\")\n",
    "            #     print(f\"Log-Likelihood: {info['log_likelihood']:.4f}\")\n",
    "            #     print(f\"Parameters: {info['parameters']}\")\n",
    "        \n",
    "            fig = plot_circular_distribution(turning_angles, bins=90)\n",
    "            fig.show()\n",
    "            \n",
    "            \n",
    "            # break\n",
    "            print(current_segment)\n",
    "            # print(original_traj_segment)\n",
    "            original_traj_segment_sequences = get_trajectory_sequences(original_traj_segment)\n",
    "            fig = plot_plotly_trajectory_groups([[current_segment], original_traj_segment_sequences], group_names=[\"AEKF\", \"Initial trajectory\"])\n",
    "            fig.show()\n",
    "            # break\n",
    "        # print(i, len(original_traj))\n",
    "        \n",
    "        # if i == 4:\n",
    "        #     print(current_segment)\n",
    "        #     print(original_traj)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
